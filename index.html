<!-- Copyright: https://pengsida.net/ -->

<!doctype html>
<html>

<head>
<title>Liang Pan</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Liang Pan, The University of Hong Kong"> 
<meta name="description" content="Liang Pan's homepage">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-137722442-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Show more content -->
<script type="text/javascript">
    function toggle_vis(id) {
        // var e = document.getElementById(id);
        var e = document.getElementsByClassName(id);
        var showText = document.getElementById("showText");
        for (var i = 0; i < e.length; i++) {
            if (e[i].style.display == "none") {
                e[i].style.display = "inline";
                showText.innerHTML = "[Show less]";
            } else {
                e[i].style.display = "none";
                showText.innerHTML = "[Show more]";
            }
        }
    }
</script>

</style>

</head>

<script>
// è·å–GitHub staræ•°é‡çš„å‡½æ•°
async function fetchGitHubStars(repo, elementId) {
    try {
        const response = await fetch(`https://api.github.com/repos/${repo}`);
        const data = await response.json();
        const stars = data.stargazers_count;
        document.getElementById(elementId).textContent = stars;
    } catch (error) {
        console.error('Error fetching GitHub stars:', error);
        document.getElementById(elementId).textContent = '?';
    }
}

// é¡µé¢åŠ è½½æ—¶è·å–staræ•°é‡
window.addEventListener('load', function() {
    fetchGitHubStars('liangpan99/TokenHSI', 'tokenhsi-stars');
    fetchGitHubStars('liangpan99/InterScene', 'interscene-stars');
});
</script>

</head>

<!-- é¼ æ ‡æ‚¬åœæ˜¾ç¤ºå›¾ç‰‡ -->
<style>
    /* å®¹å™¨æ ·å¼ */
    .hover-container {
      position: relative;
      display: inline-block;
    }
  
    /* é»˜è®¤éšè—å›¾ç‰‡ */
    .hover-image {
        display: none;
        position: absolute;
        bottom: 100%;  /* å›¾ç‰‡æ˜¾ç¤ºåœ¨æ‚¬åœå…ƒç´ ä¸Šæ–¹ */
        left: 125%;
        transform: translateX(-50%);
        z-index: 999;
        width: 200px;  /* è‡ªå®šä¹‰å›¾ç‰‡å®½åº¦ */
        border: 2px solid #ccc;
    }
  
    /* æ‚¬åœæ—¶æ˜¾ç¤ºå›¾ç‰‡ */
    .hover-trigger:hover + .hover-image {
      display: block;
    }
  </style>

</head>

<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Liang Pan æ½˜äº®<h1>
				</div>

                <h3 class="title">Building scalable AI agents with MLLMs and robots</h3>

				<p>
                    First-year Ph.D. student</br>
                    School of Computing and Data Science</br>
                    The University of Hong Kong</br>
                    
                    <div class="hover-container">
                        Email: <a href="mailto:liangpan1005@163.com">liangpan1005@163.com</a> <b>|</b>
                        <span class="hover-trigger">WeChat: liangpan_time</span>
                        <img src="images/wechat_QRcode.jpg" class="hover-image" alt="æç¤ºå›¾ç‰‡">
                    </div> </br>

					</br>
                    [<a href="https://scholar.google.com/citations?user=B5rO1jcAAAAJ&hl=en" target="_blank">Google Scholar</a>][<a href="https://github.com/liangpan99" target="_blank">Github</a>][<a href="https://x.com/liangpan_t" target="_blank">X (Twitter)</a>][<a href="https://www.linkedin.com/in/liang-pan-1327612b7/" target="_blank">Linkedin</a>][CV]</br>

                </p>

			</td>
			<td width="25%">
				<img src="images/me.jfif" width="100%" style="border-radius: 10%;"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>Biography</h2> 

<div style="display: flex">
    <p>
        I am a first-year CS Ph.D. student at The University of Hong Kong, advised by Prof. <a href="https://scholar.google.com/citations?user=TApLOhkAAAAJ&hl=en&oi=ao" target="_blank">Taku Komura</a>. 
        Prior that, I received my master's degree from Southeast University in 2024 and bachelor's degree from Hangzhou Dianzi University in 2021.
    </p>
</div>

<div style="display: flex; margin-bottom: -10px">
    <p>
       ğŸ› ï¸ Employment: <br>
        [2025/06â€”Present] Research Intern, Multi-modal Foundation Models for Humanoid Robots at <a href="https://lightrobo.com/" target="_blank">Light Robotics</a>. <br>
        [2023/09â€”2025/06] Research Intern, Shanghai AI Laboratory, working closely with Dr. <a href="https://scholar.google.com/citations?user=GStTsxAAAAAJ&amp;hl=en&amp;oi=ao" target="_blank">Jingbo Wang</a>.  <br>
        [2023/03â€”2023/09] Research Intern, Xiaohongshu Inc., Mentor: <a href="https://haofanwang.github.io/" target="_blank">Haofan Wang</a>.
    </p>


</div>

<h2>News</h2>

<ul>
    <li>
    <div>
      [2025/09/18] One paper got accepted by NeurIPS 2025.
    </div>
  </li>
    <li>
      <div>
        [2025/06/26] Three papers got accepted by ICCV 2025. See you in Hawaii! ğŸŒ´
      </div>
    </li>
    <li>
      <div>
        [2025/04/21] Invited talks (<a href="https://www.dropbox.com/scl/fi/eqkcfqtex2s9qqcycwxsj/TokenHSI.pdf?rlkey=qfu74gpiyg6rlmj9n89pim9c5&e=1&st=lkaa1uuz&dl=0" target="_blank">slides</a>) on <em>"Towards Behavior Foundation Models of Human-Scene Interactions"</em> at <a href="https://www.ea.com/?setLocale=en-us" target="_blank">Electronic Arts</a> hosted by Dr. <a href="https://hungyuling.com/" target="_blank">Ben Ling</a>, <a href="https://anysyn3d.github.io/" target="_blank">AnySyn3D</a>, and <a href="https://mp.weixin.qq.com/s/A8-8EySNhhijblxg0vau1Q" target="_blank">Shanghai Computer Society</a>.
      </div>
    </li>
    <li>
      <div>[2025/04/05] TokenHSI has been selected as an oral paper at CVPR 2025. Top 3.3% of the accepted papers (96/2878).</div>
    </li>
    <li>
      <div>[2025/03/12] A brand-new version of SIMS is available. Enjoy the stories created by SIMS!</div>
    </li>
    <li>
        <div>[2025/02/27] TokenHSI got accepted by CVPR 2025.</div>
    </li>
    
    <div class="news" style="display:none">
    </div>
</ul>

<div class="show_button">
    <a href="javascript:toggle_vis('news')" id="showText">[Show more]</a>
</div>

<h2>
    Preprints
    <!-- <span style="font-size: 65%;">(* denotes equal contribution, and <span class="corresponding">â€ </span> denotes the corresponding author)</span> -->
</h2>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/ChatDyn.png">
    </div>
    <div class="publication_title">
      <p>
        ChatDyn: Language-Driven Multi-Actor Dynamics Generation in Street Scenes</br>
        Yuxi Wei, Jingbo Wang<span class="corresponding">â€ </span>, Yuwen Du, Dingju Wang, <b>Liang Pan</b>, Chenxin Xu, Yao Feng, Bo Dai, Siheng Chen<span class="corresponding">â€ </span></br>
        [<a href="https://arxiv.org/abs/2412.08685" target="_blank">Paper</a>][<a href="https://vfishc.github.io/chatdyn/" target="_blank">Project Page</a>]
      </p>
    </div>
</div>

<h2>
    Publications
    <span style="font-size: 65%;">(* denotes equal contribution, and <span class="corresponding">â€ </span> denotes the corresponding author)</span>
</h2>

<div class="newline_bg">
    <h3>2025</h3>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/MOSPA.png">
  </div>
  <div class="publication_title">
    <p>
      ğŸ§MOSPA: Human Motion Generation Driven by Spatial Audio</br>
      Shuyang Xu*, Zhiyang Dou*<span class="corresponding">â€ </span>, Mingyi Shi, <b>Liang Pan</b>, Leo Ho, Jingbo Wang, Yuan Liu, Cheng Lin, Yuexin Ma, Wenping Wang<span class="corresponding">â€ </span>, Taku Komura<span class="corresponding">â€ </span></br>
      NeurIPS 2025 <b><span style="color:#444fe6";>ğŸŒŸ Highlight</span></b></br>
      [<a href="https://arxiv.org/abs/2507.11949" target="_blank">Paper</a>][<a href="https://frank-zy-dou.github.io/projects/MOSPA/index.html" target="_blank">Project Page</a>]
    </p>
  </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/TokenHSI.png">
        <br>
        <div style="display: flex; justify-content: center; align-items: center; margin-top: 8px;">
          <a href="https://github.com/liangpan99/TokenHSI" target="_blank" style="text-decoration: none; color: inherit; display: flex; align-items: center;">
            <svg style="width: 16px; height: 16px; vertical-align: middle; margin-right: 4px; margin-top: 1px;" viewBox="0 0 16 16" fill="currentColor">
              <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
            </svg> <span id="tokenhsi-stars">?</span>
          </a>
        </div>
    </div>
    <div class="publication_title">
      <p>
        TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</br>
        <b>Liang Pan</b>, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, Jingbo Wang<span class="corresponding">â€ </span></br>
        CVPR 2025 <b><span style="color:#f35555";>ğŸ†ï¸ Oral Presentation (Top 3.3%)</span></b></br>
        The 1st Workshop on Humanoid Agents @ CVPR 2025 <b><span style="color:#444fe6";> ğŸŒŸ Spotlight</span></b></br>
        [<a href="https://arxiv.org/abs/2503.19901" target="_blank">Paper</a>]
        [<a href="https://liangpan99.github.io/TokenHSI/" target="_blank">Project Page</a>]
      </p>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/SIMS.jpg">
    </div>
    <div class="publication_title">
      <p>
        SIMS: Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation</br>
        Wenjia Wang, <b>Liang Pan</b>, Zhiyang Dou, Jidong Mei, Zhouyingcheng Liao, Yuke Lou, Yifan Wu, Lei Yang, Jingbo Wang<span class="corresponding">â€ </span>, Taku Komura<span class="corresponding">â€ </span></br>
        ICCV 2025</br>
        [<a href="https://arxiv.org/abs/2411.19921" target="_blank">Paper</a>][<a href="https://wenjiawang0312.github.io/projects/sims/" target="_blank">Project Page</a>]
      </p>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/PlugPlayMotionRefinement.png">
    </div>
    <div class="publication_title">
      <p>
        A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions</br>
        Youliang Zhang*, Ronghui Li*, Yachao Zhang, <b>Liang Pan</b>, Jingbo Wang, Yebin Liu, Xiu Li<span class="corresponding">â€ </span></br>
        ICCV 2025 <b><span style="color:#444fe6";>ğŸŒŸ Highlight</span></b></br>
        [<a href="https://arxiv.org/abs/2412.17377" target="_blank">Paper</a>][<a href="https://physicalmotionrestoration.github.io/" target="_blank">Project Page</a>]
      </p>
    </div>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/MotionStreamer.jpg">
  </div>
  <div class="publication_title">
    <p>
      Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space</br>
      Lixing Xiao,  Shunlin Lu,  Huaijin Pi,  Ke Fan,  <strong>Liang Pan</strong>,  Yueer Zhou, Ziyong Feng, Xiaowei Zhou,  Sida Peng<span class="corresponding">â€ </span>,  Jingbo Wang</br>
      ICCV 2025</br>
      [<a href="https://arxiv.org/abs/2503.15451" target="_blank">Paper</a>][<a href="https://zju3dv.github.io/MotionStreamer/" target="_blank">Project Page</a>]
    </p>
  </div>
</div>

<div class="newline_bg">
    <h3>2024</h3>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/CloselyInteractiveHumans.jpg">
    </div>
    <div class="publication_title">
      <p>
        Closely Interactive Human Reconstruction with Proxemics and Physics-Guided Adaption</br>
        Buzhen Huang, Chen Li, Chongyang Xu, <b>Liang Pan</b>, Yangang Wang, Gim Hee Lee</br>
        CVPR 2024</br>
        [<a href="https://arxiv.org/abs/2404.11291" target="_blank">Paper</a>]
      </p>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/InterScene.jpg">
        <br>
        <div style="display: flex; justify-content: center; align-items: center; margin-top: 8px;">
          <a href="https://github.com/liangpan99/InterScene" target="_blank" style="text-decoration: none; color: inherit; display: flex; align-items: center;">
            <svg style="width: 16px; height: 16px; vertical-align: middle; margin-right: 4px; margin-top: 1px;" viewBox="0 0 16 16" fill="currentColor">
              <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
            </svg> <span id="interscene-stars">?</span>
          </a>
        </div>
    </div>
    <div class="publication_title">
      <p>
        Synthesizing Physically Plausible Human Motions in 3D Scenes</br>
        <b>Liang Pan</b>, Jingbo Wang, Buzhen Huang, Junyu Zhang, Haofan Wang, Xu Tang, Yangang Wang<span class="corresponding">â€ </span></br>
        3DV 2024</br>
        [<a href="https://arxiv.org/abs/2308.09036" target="_blank">Paper</a>][<a href="https://liangpan99.github.io/InterScene/" target="_blank">Project Page</a>]
      </p>
    </div>
</div>


<div class="newline_bg">
    <h3>2022</h3>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/NeuralMocon.jpg">
    </div>
    <div class="publication_title">
      <p>
        Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture</br>
        Buzhen Huang, <b>Liang Pan</b>, Yuan Yang, Jingyi Ju, Yangang Wang<span class="corresponding">â€ </span></br>
        CVPR 2022</br>
        [<a href="https://arxiv.org/abs/2203.14065" target="_blank">Paper</a>][<a href="https://www.yangangwang.com/papers/HBZ-NM-2022-03.html" target="_blank">Project Page</a>]
      </p>
    </div>
</div>


</div>

</div>
</body>
</html>
